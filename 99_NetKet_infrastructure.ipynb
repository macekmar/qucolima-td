{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetKet's infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"  # Before importing Jax\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jax.readthedocs.io/en/latest/\n",
    "\n",
    "Jax is \"accelerated NumPy\". It combines:\n",
    "* **XLA (accelerated linear algebra)**: can perform calculations on CPU, GPU, clusters, ...\n",
    "* **just-in-time compilation**: needs pure functions \n",
    "* **automatic differentiation**: the backbone of machine learning\n",
    "* **additional functionality**: processing PyTrees, `vmap`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jax Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jax.numpy` is almost completely the same a NumPy (e.g, you can use `jnp.sin` instead of `np.sin`).\n",
    "Jax arrays are agnostic to the device used (CPU or GPU).\n",
    "You can also mix Jax and NumPy array, altought that is not adviced because of frequent array transfers.\n",
    "There is also a lower level `jax.lax` API.\n",
    "\n",
    "The main difference is that Jax arrays are immutable.\n",
    "Think about it in this way: moving data from and to GPU is expensive.\n",
    "You want to put data to GPU and just do calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84147096 0.         0.84147096 0.84147096 0.         0.        ]\n",
      "[1.8338766  0.91905963 0.9378662  0.85064036 0.7876469  0.7818876 ]\n",
      "[ 0.99444187  0.28290102 10.          0.6057231   0.33909068  0.07518139]\n",
      "[0.99444187 0.28290102 0.86436254 0.6057231  0.33909068 0.07518139]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1701251664.922758       1 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    }
   ],
   "source": [
    "# One can easily convert Numpy array to Jax array\n",
    "jax_array = jnp.array(np.random.rand(6))\n",
    "\n",
    "# Standard numpy functions\n",
    "print(jnp.sin(jax_array > 0.5))\n",
    "\n",
    "# One can mix Jax and NumPy array (but shouldn't)\n",
    "print(jax_array + np.random.rand(6))\n",
    "\n",
    "# Jax arrays are immutable\n",
    "# jax_array[2] = 1  # -- does not work\n",
    "print(jax_array.at[2].set(10))  # This returns a copy\n",
    "print(jax_array)  # No change in original array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax JIT works with sending a \"fake\" trace array with the same size and dtype as the input array through the function.\n",
    "All array sizes have to be known at compile time.\n",
    "\n",
    "We get an error like ` Abstract tracer value encountered where concrete value is expected`\n",
    "\n",
    "Jax functions also have to be pure functions:\n",
    "- same output for the same inputs\n",
    "- no side effects (no change to inputs, no prints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting indices\n",
      "Getting values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.73435697])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @jax.jit\n",
    "def get_large_elements(x):\n",
    "  print(\"Getting indices\")\n",
    "  inds = jnp.where(x > 0.5)[0]  # -- fails here: We do not know length of inds\n",
    "  print(\"Getting values\")\n",
    "  y = x[inds]\n",
    "  return y\n",
    "\n",
    "get_large_elements(np.random.rand(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PjitFunction of <function get_large_elements at 0x1255afa30>>\n",
      "First run\n",
      "**Getting indices**\n",
      "First element of x: 0.150757\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def get_large_elements(x):\n",
    "  print(\"**Getting indices**\")  # â†’ jax.debug.print\n",
    "  y = jnp.where(x > 0.5, x, 0)  # -- Now we know size of y at compile time\n",
    "\n",
    "  # We cannot change x if jax array or compiled -- pure function\n",
    "  # print(type(x))\n",
    "  if isinstance(x, np.ndarray):\n",
    "    x[0] = 0\n",
    "  else:\n",
    "    x = x.at[0].set(.0)  # variable x is now different place in memory!\n",
    "\n",
    "  return y\n",
    "\n",
    "print(get_large_elements)\n",
    "\n",
    "print(\"First run\")\n",
    "x = np.array(np.random.rand(10))\n",
    "get_large_elements(x)\n",
    "print(f\"First element of x: {x[0]:f}\")  # -- no change to x!\n",
    "\n",
    "# jax.make_jaxpr(get_large_elements)(x)\n",
    "\n",
    "# print(\"\\nSecond run with the same-size array\")\n",
    "# get_large_elements(np.random.rand(10))\n",
    "# print(\"\\nRun with a different size array\")\n",
    "# get_large_elements(np.random.rand(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function jitted_if at /var/folders/yl/hmw36wjn7n91fv3_nqz4n34w0000gn/T/ipykernel_46230/2172352681.py:2 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m jitted_if(\u001b[39m0.7\u001b[39;49m)\n",
      "\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "\n",
      "\u001b[1;32m/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjitted_if\u001b[39m(x):\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39mif\u001b[39;00m x \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# -- problem, same for loops\u001b[39;00m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macek/Work/Teaching/NetKet_QuCoLiMa/Notebooks/99_NetKet_infrastructure.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n",
      "\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/.pyenv/HeisPic/lib/python3.10/site-packages/jax/_src/core.py:1396\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n",
      "\u001b[1;32m   1395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n",
      "\u001b[0;32m-> 1396\u001b[0m   \u001b[39mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[]..\n",
      "The error occurred while tracing the function jitted_if at /var/folders/yl/hmw36wjn7n91fv3_nqz4n34w0000gn/T/ipykernel_46230/2172352681.py:2 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "# Similar example:  (C++ would work here!)\n",
    "@jax.jit\n",
    "def jitted_if(x):\n",
    "  if x > 0:  # -- problem, same for loops\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "jitted_if(0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second example\n",
    "@jax.jit\n",
    "def jitted_if(x):\n",
    "  print(\"Compiling...\")\n",
    "  if len(x.shape) > 1:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jitted_if(jax.random.normal(jax.random.PRNGKey(0), shape=(24,1)))  # Actually, compiled every time the shape is different\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax and NetKet have several utility functions to process trees:\n",
    "* `jax.tree_map`  (used in updating parameters!)\n",
    "* `jax.tree_util.tree_reduce` \n",
    "* `netket.jax.tree_ravel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'MLP_0': {'Dense_0': {'kernel': Array([[-0.5679199 , -0.25000479, -0.33221843,  0.5674229 ,  0.48125366,\n",
       "            -0.78459463,  0.73191763, -0.10213185,  0.21821541, -0.76410572],\n",
       "           [-0.32451288, -0.06710989,  0.79808229,  0.49930144,  0.85064123,\n",
       "            -0.18705461, -0.21961051, -0.38433781, -1.10589124,  0.47019935],\n",
       "           [-0.18644778,  0.09819171,  0.28896983, -0.81544511,  0.96405829,\n",
       "            -0.24335677, -1.04170972,  0.66292027,  0.13683804,  0.67856372]],      dtype=float64),\n",
       "    'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float64)},\n",
       "   'Dense_1': {'kernel': Array([[-4.87397305e-01, -1.82726739e-01, -2.70678735e-01,\n",
       "             6.30144251e-02],\n",
       "           [-4.98449327e-01,  3.24769067e-01,  6.21437443e-01,\n",
       "             4.44547692e-01],\n",
       "           [ 3.61663103e-01,  3.77743065e-01, -2.62377913e-01,\n",
       "             3.36596660e-01],\n",
       "           [-2.67408498e-01,  3.82106092e-01,  1.60904711e-01,\n",
       "            -3.05882532e-01],\n",
       "           [ 2.00074270e-01, -3.66621725e-01,  2.76897925e-01,\n",
       "             8.96261660e-02],\n",
       "           [ 2.54328569e-01,  3.03670372e-01,  1.93757861e-01,\n",
       "             2.13896766e-01],\n",
       "           [-5.49195601e-01,  1.85315856e-01, -4.44869738e-01,\n",
       "            -1.59655415e-01],\n",
       "           [ 8.44528421e-02, -3.81810324e-01,  9.31832502e-02,\n",
       "            -3.46496634e-01],\n",
       "           [-4.70277976e-01, -5.28194770e-01,  2.46346442e-01,\n",
       "             1.77641528e-01],\n",
       "           [ 3.34291470e-01,  4.61340795e-01,  1.81818637e-01,\n",
       "             3.67326642e-05]], dtype=float64),\n",
       "    'bias': Array([0., 0., 0., 0.], dtype=float64)},\n",
       "   'Dense_2': {'kernel': Array([[-0.41205723, -0.86811209, -0.41553601, -0.28294189, -0.49364417],\n",
       "           [-0.10940891,  0.16586266, -0.46949056, -0.19018102, -0.30518224],\n",
       "           [-0.1742593 ,  0.24133139,  0.64387472,  0.18765269, -1.03575815],\n",
       "           [-0.61336182, -0.29951509,  0.16004261, -0.55474732, -0.79133077]],      dtype=float64),\n",
       "    'bias': Array([0., 0., 0., 0., 0.], dtype=float64)},\n",
       "   'Dense_3': {'kernel': Array([[-0.07866522],\n",
       "           [ 0.41071659],\n",
       "           [-0.18557742],\n",
       "           [ 0.61015305],\n",
       "           [-0.64067955]], dtype=float64)}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netket as nk\n",
    "\n",
    "model = nk.models.MLP(hidden_dims=[10,4,5])\n",
    "variables = model.init(jax.random.PRNGKey(0), jnp.ones(shape=(10,3)))\n",
    "variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are in Flax (and thus in NetKet) stored in a Python dictionary tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MLP_0'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables[\"params\"].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'MLP_0': {'Dense_0': {'bias': (10,), 'kernel': (3, 10)},\n",
       "   'Dense_1': {'bias': (4,), 'kernel': (10, 4)},\n",
       "   'Dense_2': {'bias': (5,), 'kernel': (4, 5)},\n",
       "   'Dense_3': {'kernel': (5, 1)}}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get shapes of all tree elements\n",
    "jax.tree_map(lambda leaf: leaf.shape, variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of variables in a tree\n",
    "jax.tree_util.tree_reduce(lambda acc, leaf: acc + np.prod(leaf.shape), variables, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all unraveled parameters: (114,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'MLP_0': {'Dense_0': {'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float64),\n",
       "    'kernel': Array([[-0.5679199 , -0.25000479, -0.33221843,  0.5674229 ,  0.48125366,\n",
       "            -0.78459463,  0.73191763, -0.10213185,  0.21821541, -0.76410572],\n",
       "           [-0.32451288, -0.06710989,  0.79808229,  0.49930144,  0.85064123,\n",
       "            -0.18705461, -0.21961051, -0.38433781, -1.10589124,  0.47019935],\n",
       "           [-0.18644778,  0.09819171,  0.28896983, -0.81544511,  0.96405829,\n",
       "            -0.24335677, -1.04170972,  0.66292027,  0.13683804,  0.67856372]],      dtype=float64)},\n",
       "   'Dense_1': {'bias': Array([0., 0., 0., 0.], dtype=float64),\n",
       "    'kernel': Array([[-4.87397305e-01, -1.82726739e-01, -2.70678735e-01,\n",
       "             6.30144251e-02],\n",
       "           [-4.98449327e-01,  3.24769067e-01,  6.21437443e-01,\n",
       "             4.44547692e-01],\n",
       "           [ 3.61663103e-01,  3.77743065e-01, -2.62377913e-01,\n",
       "             3.36596660e-01],\n",
       "           [-2.67408498e-01,  3.82106092e-01,  1.60904711e-01,\n",
       "            -3.05882532e-01],\n",
       "           [ 2.00074270e-01, -3.66621725e-01,  2.76897925e-01,\n",
       "             8.96261660e-02],\n",
       "           [ 2.54328569e-01,  3.03670372e-01,  1.93757861e-01,\n",
       "             2.13896766e-01],\n",
       "           [-5.49195601e-01,  1.85315856e-01, -4.44869738e-01,\n",
       "            -1.59655415e-01],\n",
       "           [ 8.44528421e-02, -3.81810324e-01,  9.31832502e-02,\n",
       "            -3.46496634e-01],\n",
       "           [-4.70277976e-01, -5.28194770e-01,  2.46346442e-01,\n",
       "             1.77641528e-01],\n",
       "           [ 3.34291470e-01,  4.61340795e-01,  1.81818637e-01,\n",
       "             3.67326642e-05]], dtype=float64)},\n",
       "   'Dense_2': {'bias': Array([0., 0., 0., 0., 0.], dtype=float64),\n",
       "    'kernel': Array([[-0.41205723, -0.86811209, -0.41553601, -0.28294189, -0.49364417],\n",
       "           [-0.10940891,  0.16586266, -0.46949056, -0.19018102, -0.30518224],\n",
       "           [-0.1742593 ,  0.24133139,  0.64387472,  0.18765269, -1.03575815],\n",
       "           [-0.61336182, -0.29951509,  0.16004261, -0.55474732, -0.79133077]],      dtype=float64)},\n",
       "   'Dense_3': {'kernel': Array([[-0.07866522],\n",
       "           [ 0.41071659],\n",
       "           [-0.18557742],\n",
       "           [ 0.61015305],\n",
       "           [-0.64067955]], dtype=float64)}}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers, unravel_fun = nk.jax.tree_ravel(variables)\n",
    "print(\"Shape of all unraveled parameters:\", numbers.shape)\n",
    "unravel_fun(numbers)  # Same structures as variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://flax.readthedocs.io/en/latest/\n",
    "\n",
    "Flax is framework based on Jax used to implement neural network models \"using functional approach\".\n",
    "\n",
    "* *jax*: Always use `jax.numpy` instead of `numpy` when defining models.\n",
    "* *functional*: The model does not store parameters (variables), it only provides information how to initialize parameters and for transformation\n",
    "$$\n",
    "f(v_{in}, x)  \\rightarrow v_{out}, y\n",
    "$$* A typical error is `Can't call compact methods on unbound modules`.\n",
    "* First axis of $x$ can be for different samples. NetKet has more than two dimensions (MPI, chunk size).\n",
    "* Variables are stored in a dictionary tree.\n",
    "* If you are using complex parameters, use NetKets linen: `netket.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'kernel': Array([[ 0.4087802 ,  0.43891278, -0.23872387, -0.8494273 ],\n",
       "         [ 0.41122693, -0.5888459 , -0.55229884,  0.49776074],\n",
       "         [ 0.3480036 , -0.7046275 , -0.30813402, -1.21659   ]],      dtype=float32),\n",
       "  'bias': Array([0., 0., 0., 0.], dtype=float32)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Dense(features=4)  # One layer FFN with 4 neurons\n",
    "\n",
    "# Initialize parameters\n",
    "# layer.init(jax.random.key(0))  # -- does not work, we need to provide intput (shape)\n",
    "x_dim = 3\n",
    "num_samples = 5\n",
    "x_in = jnp.ones((num_samples, x_dim))\n",
    "variables = layer.init(jax.random.key(0), x_in)\n",
    "\n",
    "# There are no parameters stored in the object\n",
    "# layer.variables  # -- does not work\n",
    "variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659]],      dtype=float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer(variables, x_in)  # -- does not work\n",
    "layer.apply(variables, x_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can change the number of samples in `x_in` but not the dimension of `x_in` space.\n",
    "It relies only on shape mismatch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659]],      dtype=float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.apply(variables, jnp.ones(shape=(2*num_samples, x_dim)))\n",
    "# layer.apply(variables, jnp.ones(shape=(num_samples, x_dim + 1)))  # -- does not work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save a model with specific parameters.\n",
    "NetKet never does that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659],\n",
       "       [ 1.16801071, -0.85456064, -1.09915674, -1.56825659]],      dtype=float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binded_model = layer.bind(variables)\n",
    "binded_model(x_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same functional property holds for NetKet, but we have a helper function `log_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket as nk\n",
    "\n",
    "g = nk.graph.Chain(length=4, pbc=True)\n",
    "hi = nk.hilbert.Spin(s=1/2, N=4)\n",
    "ham = nk.operator.Ising(hilbert=hi, graph=g, h=1,J=-1)\n",
    "model = nk.models.RBM(alpha=1)\n",
    "vqs = nk.vqs.FullSumState(hi, model)\n",
    "Ïƒ = vqs._all_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m  \u001b[0mvqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mÏƒ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mlog_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mÏƒ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34mr\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Evaluate the variational state for a batch of states and returns\u001b[0m\n",
      "\u001b[0;34m        the logarithm of the amplitude of the quantum state.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        For pure states, this is :math:`\\log(\\langle\\sigma|\\psi\\rangle)`,\u001b[0m\n",
      "\u001b[0;34m        whereas for mixed states\u001b[0m\n",
      "\u001b[0;34m        this is :math:`\\log(\\langle\\sigma_r|\\rho|\\sigma_c\\rangle)`, where\u001b[0m\n",
      "\u001b[0;34m        :math:`\\psi` and :math:`\\rho` are respectively a pure state\u001b[0m\n",
      "\u001b[0;34m        (wavefunction) and a mixed state (density matrix).\u001b[0m\n",
      "\u001b[0;34m        For the density matrix, the left and right-acting states (row and column)\u001b[0m\n",
      "\u001b[0;34m        are obtained as :code:`Ïƒr=Ïƒ[::,0:N]` and :code:`Ïƒc=Ïƒ[::,N:]`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Given a batch of inputs :code:`(Nb, N)`, returns a batch of outputs\u001b[0m\n",
      "\u001b[0;34m        :code:`(Nb,)`.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mjit_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mÏƒ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/HeisPic/lib/python3.10/site-packages/netket/vqs/full_summ/state.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "vqs.log_value(Ïƒ)\n",
    "?? vqs.log_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to implement a model is to subclass `flax.linen.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class TwoLayerFFN(nn.Module):\n",
    "  # Here we define model's parameters\n",
    "  # We have to provide typing if we want to change values at initialization\n",
    "  param_dtype: Any = jnp.float64\n",
    "  num_features_1: int = 4\n",
    "  num_features_2: int = 3\n",
    "  use_bias: Any = True\n",
    "  activation_fun: Any = nn.relu\n",
    "\n",
    "  # nn.compact decorator provides a compact way of defining a model\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # Let us do first layer using Flax:\n",
    "    x = nn.Dense(name=\"Layer1\", features=self.num_features_1, use_bias=self.use_bias, dtype=self.param_dtype)(x)\n",
    "    x = self.activation_fun(x)\n",
    "\n",
    "    # And second layer by hand\n",
    "    # We have to provide 4 variables:\n",
    "    #   - parameter name\n",
    "    #   - initializer function (nn.initializers.normal() returns a function)\n",
    "    #     initializer takes two inputs: random gen key and shape.\n",
    "    #     We provide the former when we call mode.init(...)\n",
    "    #   - tensor shape\n",
    "    #   - tensor dtype\n",
    "    W  = self.param(\"Layer2/Weights\", nn.initializers.normal(), (self.num_features_1, self.num_features_2), self.param_dtype)  # Not shape= _2, _1 [*]\n",
    "    b = self.param(\"Layer2/Bias\", nn.initializers.normal(), (self.num_features_2,), self.param_dtype)\n",
    "    x = x@W + b\n",
    "    x = self.activation_fun(x)  # First dimension of x can be num_of_samples [*]\n",
    "\n",
    "    # Finally we just sum outputs from the second layer\n",
    "    return jnp.sum(x, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macek/.pyenv/HeisPic/lib/python3.10/site-packages/flax/linen/dtypes.py:97: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return [jnp.asarray(x, dtype) if x is not None else None for x in args]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([0.01679351, 0.01679351, 0.01679351, 0.01679351, 0.01679351,\n",
       "       0.01679351], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TwoLayerFFN(num_features_1=6)  # activation_fun=jnp.sin\n",
    "variables = model.init(jax.random.PRNGKey(0), jnp.ones(shape=(10,3)))\n",
    "\n",
    "model.apply(variables, jnp.ones(shape=(6,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://optax.readthedocs.io/en/latest/\n",
    "\n",
    "For the moment, NetKet's optimizer suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plum â€“ multiple dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Polymorphism where function overloading happens at runtime and not at compilation time\"\n",
    "\n",
    "Python already has single dispatch (function is decided on the first argument, e.g. `self`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plum import dispatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No idea\n",
      "float/int: 2.15\n",
      "int/int: 2\n"
     ]
    }
   ],
   "source": [
    "@dispatch\n",
    "def div(x, y):\n",
    "  print(\"No idea\")\n",
    "\n",
    "@dispatch\n",
    "def div(x: float, y: int):\n",
    "  print(f\"float/int: {x/y}\")\n",
    "\n",
    "@dispatch\n",
    "def div(x: int, y: int):\n",
    "  print(f\"int/int: {x//y}\")\n",
    "\n",
    "div(2.3, 4.0)\n",
    "div(4.3, 2)\n",
    "div(5, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HeisPic",
   "language": "python",
   "name": "heispic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
